# Story 3.1: AI Key Insight Generation

## Status
Approved

## Story
**As a** user,
**I want to** see a single, critical insight on each task automatically,
**so that** I can get immediate value and context without any extra clicks.

## Acceptance Criteria
1. A backend service is created that sends task data (title, description) to the Perplexity API for analysis.
2. The service successfully processes the API response to extract a single, concise insight.
3. This key insight is stored and associated with the corresponding task in the Syntaskio database.
4. The key insight is displayed by default on each task card in the Unified Task Dashboard UI.

## Tasks / Subtasks
- [ ] Task 1: Set up Perplexity AI integration (AC: 1)
  - [ ] Research Perplexity API authentication and endpoints
  - [ ] Configure Perplexity API credentials and environment setup
  - [ ] Create Perplexity API client service with error handling
  - [ ] Implement rate limiting and cost management for AI calls
- [ ] Task 2: Design AI insight prompting strategy (AC: 1, 2)
  - [ ] Create optimized prompts for key insight generation
  - [ ] Implement task context preparation and sanitization
  - [ ] Add insight validation and quality filtering
  - [ ] Create fallback strategies for low-quality insights
- [ ] Task 3: Implement AI insight service (AC: 1, 2, 3)
  - [ ] Create AI insight generation service
  - [ ] Implement async insight processing pipeline
  - [ ] Add insight storage and caching mechanisms
  - [ ] Create insight refresh and update logic
- [ ] Task 4: Extend database schema for AI insights (AC: 3)
  - [ ] Add AI insights table with task relationships
  - [ ] Implement insight versioning and history tracking
  - [ ] Add insight metadata (confidence, generation time, cost)
  - [ ] Create database indexes for efficient insight retrieval
- [ ] Task 5: Integrate AI insights with task aggregation (AC: 3)
  - [ ] Add insight generation to task creation pipeline
  - [ ] Implement background insight generation for existing tasks
  - [ ] Create insight regeneration for updated tasks
  - [ ] Add insight performance monitoring and optimization
- [ ] Task 6: Update frontend for AI insight display (AC: 4)
  - [ ] Design insight display component for task cards
  - [ ] Update TaskCard component with AI insight integration
  - [ ] Add insight loading states and error handling
  - [ ] Create insight refresh and regeneration controls
- [ ] Task 7: Create comprehensive testing (AC: 1-4)
  - [ ] Write unit tests for AI service and insight processing
  - [ ] Write integration tests for Perplexity API integration
  - [ ] Create E2E tests for complete insight generation workflow
  - [ ] Test AI insight display and user interactions
  - [ ] Performance testing with batch insight generation

## Dev Notes

### Previous Story Insights
From Stories 1.1-2.4, the foundation provides:
- Complete authentication system with GCP Identity Platform
- Microsoft Todo, ConnectWise Manage, and Process Plan integrations
- Unified Task Dashboard with multi-source task display
- Task aggregation service with scheduled synchronization
- Bi-directional sync service with queue-based processing
- Smart task de-duplication with merged task support
- Comprehensive task data models supporting all integration sources

### Perplexity AI Integration Overview
[Source: Perplexity AI API Documentation]
- **Base URL**: `https://api.perplexity.ai`
- **Authentication**: Bearer token authentication
- **Models**: `llama-3.1-sonar-small-128k-online`, `llama-3.1-sonar-large-128k-online`
- **Rate Limits**: 20 requests per minute (standard), 200 requests per minute (pro)
- **Cost Structure**: $1-5 per 1M tokens depending on model
- **Features**: Real-time web search integration, citations, structured responses

### AI Insight Generation Strategy
[Source: AI Enhancement Requirements from Epic 3]

#### Insight Types and Prompting
1. **Quick Context**: Brief explanation of task purpose or importance
2. **Action Recommendations**: Suggested next steps or approaches
3. **Related Information**: Relevant context from web search
4. **Time Estimates**: Realistic completion time suggestions
5. **Dependencies**: Potential blockers or prerequisites

#### Optimized Prompts
```typescript
const INSIGHT_PROMPTS = {
  general: `Analyze this task and provide a single, actionable insight in 1-2 sentences:
Title: {title}
Description: {description}
Source: {source}
Priority: {priority}

Focus on: immediate value, next steps, or key context. Be concise and specific.`,

  technical: `For this technical task, provide a brief insight about approach or considerations:
Title: {title}
Description: {description}
Technology context: {sourceData}

Suggest: best practices, potential challenges, or implementation approach.`,

  process: `For this process/workflow task, provide insight about execution or optimization:
Title: {title}
Description: {description}
Process context: {processInfo}

Focus on: efficiency tips, common pitfalls, or success factors.`
};
```

### AI Insight Data Model
```typescript
interface AIInsight {
  id: string;
  taskId: string;
  userId: string;
  insight: string;
  confidence: number; // 0-1 confidence score
  model: string; // Perplexity model used
  promptVersion: string;
  generatedAt: Date;
  tokensUsed: number;
  cost: number; // Cost in USD
  citations?: string[]; // URLs or sources referenced
  metadata: {
    taskTitle: string;
    taskSource: string;
    promptType: string;
    processingTime: number;
  };
  status: 'generating' | 'completed' | 'failed' | 'refreshing';
  error?: string;
  createdAt: Date;
  updatedAt: Date;
}

interface InsightGenerationJob {
  id: string;
  taskId: string;
  userId: string;
  priority: 'high' | 'normal' | 'low';
  attempts: number;
  maxAttempts: number;
  status: 'pending' | 'processing' | 'completed' | 'failed';
  scheduledAt: Date;
  processedAt?: Date;
  error?: string;
}
```

### Database Schema Updates
```sql
-- AI insights table
CREATE TABLE ai_insights (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    insight TEXT NOT NULL,
    confidence NUMERIC(3,2),
    model TEXT NOT NULL,
    prompt_version TEXT NOT NULL,
    generated_at TIMESTAMPTZ NOT NULL,
    tokens_used INTEGER,
    cost NUMERIC(10,6),
    citations JSONB,
    metadata JSONB,
    status TEXT DEFAULT 'completed',
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    UNIQUE(task_id) -- One current insight per task
);

-- Insight generation queue
CREATE TABLE insight_generation_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    priority TEXT DEFAULT 'normal',
    attempts INTEGER DEFAULT 0,
    max_attempts INTEGER DEFAULT 3,
    status TEXT DEFAULT 'pending',
    scheduled_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    processed_at TIMESTAMPTZ,
    error TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Insight history for versioning
CREATE TABLE ai_insight_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    insight_id UUID NOT NULL REFERENCES ai_insights(id) ON DELETE CASCADE,
    insight TEXT NOT NULL,
    confidence NUMERIC(3,2),
    model TEXT NOT NULL,
    generated_at TIMESTAMPTZ NOT NULL,
    archived_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

-- Indexes for performance
CREATE INDEX idx_ai_insights_task_id ON ai_insights(task_id);
CREATE INDEX idx_ai_insights_user_id ON ai_insights(user_id);
CREATE INDEX idx_insight_jobs_status ON insight_generation_jobs(status);
CREATE INDEX idx_insight_jobs_scheduled_at ON insight_generation_jobs(scheduled_at);
```

### Insight Generation Pipeline
[Source: architecture/backend-architecture.md]

#### Real-Time Generation
1. **Task Creation**: Generate insights for new tasks immediately
2. **High-Priority Queue**: Process urgent tasks (user-requested regeneration)
3. **Rate Limiting**: Respect Perplexity API limits with intelligent queuing
4. **Error Handling**: Graceful fallback for API failures

#### Background Processing
1. **Batch Generation**: Process existing tasks without insights
2. **Refresh Strategy**: Regenerate insights for updated tasks
3. **Cost Optimization**: Batch requests and model selection based on complexity
4. **Quality Monitoring**: Track insight quality and user engagement

#### Insight Quality Control
```typescript
interface InsightQuality {
  length: boolean; // 10-200 characters
  relevance: boolean; // Contains task-related keywords
  actionability: boolean; // Includes actionable language
  confidence: number; // Model confidence score
  citations: boolean; // Has supporting sources
}

const validateInsight = (insight: string, task: Task): InsightQuality => {
  // Quality validation logic
};
```

### Cost Management and Optimization
[Source: architecture/security-and-performance.md]

#### Cost Control Strategies
- **Model Selection**: Use smaller models for simple tasks, larger for complex
- **Caching**: Cache insights to avoid regeneration for similar tasks
- **Batching**: Group multiple task analyses in single API calls
- **Rate Limiting**: Intelligent queuing to respect API limits and costs
- **User Limits**: Per-user monthly insight generation limits

#### Estimated Costs
- **Small Model**: ~$0.001 per insight (1000 tokens average)
- **Large Model**: ~$0.005 per insight (1000 tokens average)
- **Monthly Budget**: $50-200 per 1000 active users
- **Optimization Target**: <$0.10 per user per month

### Frontend AI Insight Display
[Source: architecture/frontend-architecture.md]

#### TaskCard Integration
- **Insight Section**: Dedicated area below task title/description
- **Loading States**: Skeleton loading while insights generate
- **Error Handling**: Fallback display for failed insight generation
- **Refresh Control**: Manual regeneration button for users

#### Visual Design
- **Icon Indicator**: AI/lightbulb icon to identify AI-generated content
- **Typography**: Italicized or distinct styling for insights
- **Expandable**: Click to expand for full insight text
- **Source Attribution**: Clear indication of AI generation

### Performance Requirements
- **Generation Time**: 95% of insights generated within 30 seconds
- **UI Responsiveness**: Insights load asynchronously without blocking
- **API Efficiency**: Batch processing for multiple tasks
- **Cache Hit Rate**: 60%+ cache hit rate for similar tasks

### Security and Privacy
[Source: architecture/security-and-performance.md]
- **Data Sanitization**: Remove sensitive information before API calls
- **API Key Security**: Secure storage and rotation of Perplexity API keys
- **User Consent**: Clear disclosure of AI insight generation
- **Data Retention**: Configurable insight retention periods
- **Audit Logging**: Track all AI API calls and costs

### File Locations
Based on established service patterns:
- AI service: `apps/api/src/ai/`
- Perplexity client: `apps/api/src/ai/perplexity.service.ts`
- Insight service: `apps/api/src/ai/insight.service.ts`
- Insight controller: `apps/api/src/ai/insight.controller.ts`
- Insight queue: `apps/api/src/ai/insight-queue.service.ts`
- Frontend components: `apps/web/src/components/ai/`
- Insight display: `apps/web/src/components/ai/TaskInsight.tsx`
- AI types: `packages/shared-types/src/ai.ts`

### Technical Constraints
- TypeScript 5.5.x must be used consistently across all packages
- Perplexity API rate limits must be respected (20-200 requests/minute)
- AI insight generation must not block task creation or updates
- All AI API calls must be tracked for cost monitoring
- Insights must be generated asynchronously with proper error handling
- User privacy must be maintained in AI processing

### Integration Points
- **Task Aggregation Service**: Trigger insight generation for new tasks
- **Task Update Events**: Regenerate insights when tasks are modified
- **Background Jobs**: Scheduled insight generation and refresh
- **Cost Monitoring**: Track and alert on AI API usage and costs
- **WebSocket Service**: Real-time insight delivery to connected clients

## Testing
[Source: architecture/testing-coding-error-handling-and-monitoring-standards.md]
- **Unit Tests**: AI service logic, insight processing, quality validation
- **Integration Tests**: Perplexity API integration, insight storage and retrieval
- **E2E Tests**: Complete insight generation and display workflow
- **Performance Tests**: Batch insight generation, API rate limiting
- **Cost Tests**: Validate cost tracking and budget controls
- **Quality Tests**: Insight relevance and actionability validation

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-02 | 1.0 | Initial story creation | Claude Code |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled during implementation*

### Debug Log References
*To be filled during implementation*

### Completion Notes List
*To be filled during implementation*

### File List
*To be filled during implementation*

## QA Results
*Results from QA Agent review will be populated here after implementation*